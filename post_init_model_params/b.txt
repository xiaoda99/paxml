_hparams.acc_batch_mean : False
_hparams.activation_split_dims_mapping.out : NoneType
_hparams.apply_eval_sample_weights : False
_hparams.cls : type/praxis.layers.models/LanguageModel
_hparams.contiguous_submeshes : NoneType
_hparams.count_tokens : False
_hparams.dcn_mesh_shape : (1, 1, 1)
_hparams.decoder_tpl.decode_loop_mesh_axes_transpose : NoneType
_hparams.decoder_tpl.emb_lookup_style : 'matmul'
_hparams.decoder_tpl.eos_id : 1
_hparams.decoder_tpl.fprop_for_prefix : False
_hparams.decoder_tpl.lazy_prefix_broadcast : False
_hparams.decoder_tpl.max_decode_steps : NoneType
_hparams.decoder_tpl.min_prefix_len : 5
_hparams.decoder_tpl.process_result_fn : NoneType
_hparams.decoder_tpl.seqlen : 2049
_hparams.dtype : type/jax.numpy/float32
_hparams.fprop_dtype : dtype[bfloat16]
_hparams.ici_mesh_shape : (1, 16, 2)
_hparams.lm_tpl : NoneType
_hparams.mesh_axis_names : ('replica', 'data', 'mdl')
_hparams.model_type : 'causal'
_hparams.name : 'xformer_lm'
_hparams.params_init.method : 'gaussian'
_hparams.params_init.scale : 0.013975424859373685
_hparams.report_strict_acc : False
_hparams.return_predictions : False
_hparams.shared_weight_layer_id : NoneType
_hparams.skip_lp_regularization : NoneType
_hparams.weight_split_dims_mapping.wt : NoneType
lm._hparams.activation_split_dims_mapping.out : NoneType
lm._hparams.cls : type/praxis.layers.transformer_models/TransformerLm
lm._hparams.contiguous_submeshes : NoneType
lm._hparams.dcn_mesh_shape : (1, 1, 1)
lm._hparams.dtype : type/jax.numpy/float32
lm._hparams.early_stacked_transformer_tpl : NoneType
lm._hparams.final_ln_tpl : NoneType
lm._hparams.fprop_dtype : dtype[bfloat16]
lm._hparams.ici_mesh_shape : (1, 16, 2)
lm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm._hparams.model_dims : 2048
lm._hparams.model_type : 'causal'
lm._hparams.name : 'lm'
lm._hparams.ngrammer_tpl : NoneType
lm._hparams.packed_input : True
lm._hparams.params_init.method : 'gaussian'
lm._hparams.params_init.scale : 0.013975424859373685
lm._hparams.position_emb_tpl : NoneType
lm._hparams.post_attention_ngrammer_tpls : NoneType
lm._hparams.record_activations_in_xent_output : False
lm._hparams.separate_embedding_tpl : NoneType
lm._hparams.shared_weight_layer_id : NoneType
lm._hparams.skip_aux_loss : False
lm._hparams.skip_compute_loss : False
lm._hparams.skip_lp_regularization : NoneType
lm._hparams.softmax_tpl : NoneType
lm._hparams.stacked_transformer_tpl : NoneType
lm._hparams.vocab_size : 50432
lm._hparams.weight_split_dims_mapping.wt : NoneType
lm.embedding_lookup._hparams.activation_split_dims_mapping.emb_out_split_dims_mapping : [('replica', 'data'), 'NoneType', 'mdl']
lm.embedding_lookup._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.embedding_lookup._hparams.array_lookup_tpl : NoneType
lm.embedding_lookup._hparams.cls : type/praxis.layers.embedding_softmax/Embedding
lm.embedding_lookup._hparams.contiguous_submeshes : NoneType
lm.embedding_lookup._hparams.dcn_mesh_shape : (1, 1, 1)
lm.embedding_lookup._hparams.dtype : type/jax.numpy/float32
lm.embedding_lookup._hparams.einsum_tpl : NoneType
lm.embedding_lookup._hparams.fprop_dtype : dtype[bfloat16]
lm.embedding_lookup._hparams.ici_mesh_shape : (1, 16, 2)
lm.embedding_lookup._hparams.input_dims : 2048
lm.embedding_lookup._hparams.lookup_style : 'index'
lm.embedding_lookup._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.embedding_lookup._hparams.name : 'embedding_lookup'
lm.embedding_lookup._hparams.num_classes : 50432
lm.embedding_lookup._hparams.params_init.method : 'gaussian'
lm.embedding_lookup._hparams.params_init.scale : 0.013975424859373685
lm.embedding_lookup._hparams.scale_sqrt_depth : False
lm.embedding_lookup._hparams.set_nan_for_oob_id : False
lm.embedding_lookup._hparams.shared_weight_layer_id : NoneType
lm.embedding_lookup._hparams.skip_lp_regularization : NoneType
lm.embedding_lookup._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.embedding_lookup.array_lookup._hparams.activation_split_dims_mapping.out : NoneType
lm.embedding_lookup.array_lookup._hparams.cls : type/praxis.layers.base_ops/ArrayLookup
lm.embedding_lookup.array_lookup._hparams.contiguous_submeshes : NoneType
lm.embedding_lookup.array_lookup._hparams.dcn_mesh_shape : (1, 1, 1)
lm.embedding_lookup.array_lookup._hparams.dtype : type/jax.numpy/float32
lm.embedding_lookup.array_lookup._hparams.fprop_dtype : dtype[bfloat16]
lm.embedding_lookup.array_lookup._hparams.ici_mesh_shape : (1, 16, 2)
lm.embedding_lookup.array_lookup._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.embedding_lookup.array_lookup._hparams.name : 'array_lookup'
lm.embedding_lookup.array_lookup._hparams.params_init.method : 'gaussian'
lm.embedding_lookup.array_lookup._hparams.params_init.scale : 0.013975424859373685
lm.embedding_lookup.array_lookup._hparams.shared_weight_layer_id : NoneType
lm.embedding_lookup.array_lookup._hparams.skip_lp_regularization : NoneType
lm.embedding_lookup.array_lookup._hparams.weight_split_dims_mapping.wt : NoneType
lm.final_ln._hparams.activation_split_dims_mapping.out : NoneType
lm.final_ln._hparams.cls : type/praxis.layers.normalizations/LayerNorm
lm.final_ln._hparams.contiguous_submeshes : NoneType
lm.final_ln._hparams.dcn_mesh_shape : (1, 1, 1)
lm.final_ln._hparams.dim : 2048
lm.final_ln._hparams.dtype : type/jax.numpy/float32
lm.final_ln._hparams.epsilon : 1e-05
lm.final_ln._hparams.fprop_dtype : dtype[bfloat16]
lm.final_ln._hparams.ici_mesh_shape : (1, 16, 2)
lm.final_ln._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.final_ln._hparams.name : 'final_ln'
lm.final_ln._hparams.params_init.method : 'gaussian'
lm.final_ln._hparams.params_init.scale : 0.013975424859373685
lm.final_ln._hparams.reductions_in_fp32 : False
lm.final_ln._hparams.shared_weight_layer_id : NoneType
lm.final_ln._hparams.skip_lp_regularization : NoneType
lm.final_ln._hparams.use_bias : True
lm.final_ln._hparams.use_scale : True
lm.final_ln._hparams.weight_split_dims_mapping.wt : NoneType
lm.softmax._hparams.activation_split_dims_mapping.out : NoneType
lm.softmax._hparams.bi_tempered_loss_tpl : NoneType
lm.softmax._hparams.bias_init : 0.0
lm.softmax._hparams.chunk_size : NoneType
lm.softmax._hparams.cls : type/praxis.layers.embedding_softmax/FullSoftmax
lm.softmax._hparams.contiguous_submeshes : NoneType
lm.softmax._hparams.dcn_mesh_shape : (1, 1, 1)
lm.softmax._hparams.dtype : type/jax.numpy/float32
lm.softmax._hparams.feed_forward_tpl : NoneType
lm.softmax._hparams.fprop_dtype : dtype[bfloat16]
lm.softmax._hparams.ici_mesh_shape : (1, 16, 2)
lm.softmax._hparams.input_dims : 2048
lm.softmax._hparams.label_smoothing_apply_for_eval : True
lm.softmax._hparams.label_smoothing_prob : 0.0
lm.softmax._hparams.loss_batch_mean : False
lm.softmax._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.softmax._hparams.name : 'softmax'
lm.softmax._hparams.num_classes : 50432
lm.softmax._hparams.params_init.method : 'gaussian'
lm.softmax._hparams.params_init.scale : 0.013975424859373685
lm.softmax._hparams.shared_weight_layer_id : NoneType
lm.softmax._hparams.skip_lp_regularization : NoneType
lm.softmax._hparams.soft_cap_logits : NoneType
lm.softmax._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.softmax._hparams.z_loss_weight : 0.0
lm.softmax.logits_ffn._hparams.activation_split_dims_mapping.out : NoneType
lm.softmax.logits_ffn._hparams.activation_tpl : NoneType
lm.softmax.logits_ffn._hparams.bias_init : 0.0
lm.softmax.logits_ffn._hparams.bias_tpl : NoneType
lm.softmax.logits_ffn._hparams.cls : type/praxis.layers.linears/FeedForward
lm.softmax.logits_ffn._hparams.contiguous_submeshes : NoneType
lm.softmax.logits_ffn._hparams.dcn_mesh_shape : (1, 1, 1)
lm.softmax.logits_ffn._hparams.dtype : type/jax.numpy/float32
lm.softmax.logits_ffn._hparams.fprop_dtype : dtype[bfloat16]
lm.softmax.logits_ffn._hparams.has_bias : False
lm.softmax.logits_ffn._hparams.ici_mesh_shape : (1, 16, 2)
lm.softmax.logits_ffn._hparams.input_dims : 2048
lm.softmax.logits_ffn._hparams.linear_tpl : NoneType
lm.softmax.logits_ffn._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.softmax.logits_ffn._hparams.name : 'logits_ffn'
lm.softmax.logits_ffn._hparams.output_dims : 50432
lm.softmax.logits_ffn._hparams.params_init.method : 'gaussian'
lm.softmax.logits_ffn._hparams.params_init.scale : 0.013975424859373685
lm.softmax.logits_ffn._hparams.shared_weight_layer_id : NoneType
lm.softmax.logits_ffn._hparams.skip_lp_regularization : NoneType
lm.softmax.logits_ffn._hparams.weight_init : NoneType
lm.softmax.logits_ffn._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.softmax.logits_ffn.activation._hparams.activation_split_dims_mapping.out : NoneType
lm.softmax.logits_ffn.activation._hparams.cls : type/praxis.layers.activations/Identity
lm.softmax.logits_ffn.activation._hparams.contiguous_submeshes : NoneType
lm.softmax.logits_ffn.activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.softmax.logits_ffn.activation._hparams.dtype : type/jax.numpy/float32
lm.softmax.logits_ffn.activation._hparams.fprop_dtype : dtype[bfloat16]
lm.softmax.logits_ffn.activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.softmax.logits_ffn.activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.softmax.logits_ffn.activation._hparams.name : 'activation'
lm.softmax.logits_ffn.activation._hparams.params_init.method : 'gaussian'
lm.softmax.logits_ffn.activation._hparams.params_init.scale : 0.013975424859373685
lm.softmax.logits_ffn.activation._hparams.shared_weight_layer_id : NoneType
lm.softmax.logits_ffn.activation._hparams.skip_lp_regularization : NoneType
lm.softmax.logits_ffn.activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.softmax.logits_ffn.linear._hparams.activation_split_dims_mapping.out : NoneType
lm.softmax.logits_ffn.linear._hparams.cls : type/praxis.layers.linears/Linear
lm.softmax.logits_ffn.linear._hparams.contiguous_submeshes : NoneType
lm.softmax.logits_ffn.linear._hparams.dcn_mesh_shape : (1, 1, 1)
lm.softmax.logits_ffn.linear._hparams.dtype : type/jax.numpy/float32
lm.softmax.logits_ffn.linear._hparams.einsum_tpl : NoneType
lm.softmax.logits_ffn.linear._hparams.fprop_dtype : dtype[bfloat16]
lm.softmax.logits_ffn.linear._hparams.ici_mesh_shape : (1, 16, 2)
lm.softmax.logits_ffn.linear._hparams.input_dims : 2048
lm.softmax.logits_ffn.linear._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.softmax.logits_ffn.linear._hparams.name : 'linear'
lm.softmax.logits_ffn.linear._hparams.output_dims : 50432
lm.softmax.logits_ffn.linear._hparams.params_init.method : 'gaussian'
lm.softmax.logits_ffn.linear._hparams.params_init.scale : 0.013975424859373685
lm.softmax.logits_ffn.linear._hparams.shared_weight_layer_id : NoneType
lm.softmax.logits_ffn.linear._hparams.skip_lp_regularization : NoneType
lm.softmax.logits_ffn.linear._hparams.weight_init : NoneType
lm.softmax.logits_ffn.linear._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.softmax.logits_ffn.linear.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.softmax.logits_ffn.linear.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.softmax.logits_ffn.linear.einsum._hparams.contiguous_submeshes : NoneType
lm.softmax.logits_ffn.linear.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.softmax.logits_ffn.linear.einsum._hparams.dtype : type/jax.numpy/float32
lm.softmax.logits_ffn.linear.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.softmax.logits_ffn.linear.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.softmax.logits_ffn.linear.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.softmax.logits_ffn.linear.einsum._hparams.name : 'einsum'
lm.softmax.logits_ffn.linear.einsum._hparams.params_init.method : 'gaussian'
lm.softmax.logits_ffn.linear.einsum._hparams.params_init.scale : 0.013975424859373685
lm.softmax.logits_ffn.linear.einsum._hparams.shared_weight_layer_id : NoneType
lm.softmax.logits_ffn.linear.einsum._hparams.skip_lp_regularization : NoneType
lm.softmax.logits_ffn.linear.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer._hparams.block : NoneType
lm.transformer._hparams.checkpoint_policy : 'save_nothing'
lm.transformer._hparams.cls : type/praxis.layers.transformers/StackedTransformerRepeated
lm.transformer._hparams.contiguous_submeshes : NoneType
lm.transformer._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer._hparams.dtype : type/jax.numpy/float32
lm.transformer._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer._hparams.name : 'transformer'
lm.transformer._hparams.nd_prefix_shape : NoneType
lm.transformer._hparams.params_init.method : 'gaussian'
lm.transformer._hparams.params_init.scale : 0.013975424859373685
lm.transformer._hparams.repeat_layer_name : 'repeat'
lm.transformer._hparams.repeat_optimizer_dims_mapping : NoneType
lm.transformer._hparams.shared_weight_layer_id : NoneType
lm.transformer._hparams.skip_lp_regularization : NoneType
lm.transformer._hparams.sublayer_name : 'sub'
lm.transformer._hparams.unroll_in_decode : True
lm.transformer._hparams.weight_split_dims_mapping.block : NoneType
lm.transformer._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer._hparams.x_times : 12
lm.transformer.repeat._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat._hparams.checkpoint_policy : 'save_nothing'
lm.transformer.repeat._hparams.cls : type/praxis.layers.repeats/Repeat
lm.transformer.repeat._hparams.collect_intermediate_outputs : False
lm.transformer.repeat._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat._hparams.name : 'repeat'
lm.transformer.repeat._hparams.nd_prefix_shape : NoneType
lm.transformer.repeat._hparams.optimizer_dims_mapping : NoneType
lm.transformer.repeat._hparams.params_init.method : 'gaussian'
lm.transformer.repeat._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat._hparams.positional_args_as_scan_carry : False
lm.transformer.repeat._hparams.return_intermediate_outputs : False
lm.transformer.repeat._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat._hparams.sub_tpl : NoneType
lm.transformer.repeat._hparams.sublayer_name : 'sub'
lm.transformer.repeat._hparams.unpack_summaries : True
lm.transformer.repeat._hparams.unroll_in_decode : True
lm.transformer.repeat._hparams.weight_split_dims_mapping.sub : NoneType
lm.transformer.repeat._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat._hparams.x_times : 12
lm.transformer.repeat.sub._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub._hparams.atten_dropout_prob : NoneType
lm.transformer.repeat.sub._hparams.checkpoint_policy : 'save_dot_except_logits_ffn1'
lm.transformer.repeat.sub._hparams.cls : type/praxis.layers.transformers/StackedTransformer
lm.transformer.repeat.sub._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub._hparams.dim_per_head : 128
lm.transformer.repeat.sub._hparams.dropout_prob : 0.0
lm.transformer.repeat.sub._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub._hparams.fold_padding_with_segment_mask : True
lm.transformer.repeat.sub._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub._hparams.gating_func : 'top2'
lm.transformer.repeat.sub._hparams.hidden_dims : 8192
lm.transformer.repeat.sub._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub._hparams.input_dropout_prob : 0.0
lm.transformer.repeat.sub._hparams.mask_self_attention : True
lm.transformer.repeat.sub._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub._hparams.min_group_size : NoneType
lm.transformer.repeat.sub._hparams.model_dims : 2048
lm.transformer.repeat.sub._hparams.moe_layer_tpl : NoneType
lm.transformer.repeat.sub._hparams.name : 'sub'
lm.transformer.repeat.sub._hparams.ngrammer_tpls : NoneType
lm.transformer.repeat.sub._hparams.num_experts : 0
lm.transformer.repeat.sub._hparams.num_groups : 1
lm.transformer.repeat.sub._hparams.num_heads : 16
lm.transformer.repeat.sub._hparams.num_layers : 2
lm.transformer.repeat.sub._hparams.packed_input : True
lm.transformer.repeat.sub._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub._hparams.relu_dropout_prob : NoneType
lm.transformer.repeat.sub._hparams.remat : False
lm.transformer.repeat.sub._hparams.residual_dropout_prob : NoneType
lm.transformer.repeat.sub._hparams.residual_droppath_prob : 0.0
lm.transformer.repeat.sub._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub._hparams.transformer_layer_params_tpl : NoneType
lm.transformer.repeat.sub._hparams.unadjusted_expert_capacity_factor : 2.0
lm.transformer.repeat.sub._hparams.use_cross_attention : False
lm.transformer.repeat.sub._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.allow_skip_cross_attention : False
lm.transformer.repeat.sub.x_layers_0._hparams.atten_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_0._hparams.cls : type/praxis.layers.transformers/Transformer
lm.transformer.repeat.sub.x_layers_0._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.cross_atten_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_0._hparams.dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0._hparams.gpt_j_residual : True
lm.transformer.repeat.sub.x_layers_0._hparams.hidden_dims : 8192
lm.transformer.repeat.sub.x_layers_0._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_0._hparams.ln_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0._hparams.name : 'x_layers_0'
lm.transformer.repeat.sub.x_layers_0._hparams.ngrammer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.norm_policy : 'pre'
lm.transformer.repeat.sub.x_layers_0._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0._hparams.packed_input : True
lm.transformer.repeat.sub.x_layers_0._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0._hparams.relu_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_0._hparams.residual_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_0._hparams.residual_droppath_prob : 0.0
lm.transformer.repeat.sub.x_layers_0._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.tr_atten_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.tr_fflayer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0._hparams.use_cross_attention : False
lm.transformer.repeat.sub.x_layers_0._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.activation_split_dims_mapping.ffn0 : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.activation_split_dims_mapping.ffn1 : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.activation_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.add_skip_connection : False
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.apply_padding_first : False
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.chunk_size : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.cls : type/praxis.layers.transformers/TransformerFeedForward
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.fflayer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.has_bias : True
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.hidden_dims : 8192
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.internal_gshard_variance_scaling_fan_in_init : False
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.ln_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.name : 'ff_layer'
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.norm_policy : 'pre'
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.output_dims : 0
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.output_layer_std : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.relu_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.relu_dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.residual_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.residual_dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.residual_droppath_prob : 0.0
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.residual_weight : 1.0
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.use_gated_activation : False
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.weight_split_dims_mapping.ffn0 : ['data', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.weight_split_dims_mapping.ffn1 : ['mdl', 'data']
lm.transformer.repeat.sub.x_layers_0.ff_layer._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.activation_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.bias_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.cls : type/praxis.layers.linears/FeedForward
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.has_bias : True
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.linear_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.name : 'ffn_layer1'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.output_dims : 8192
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.approximate : True
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.name : 'activation'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.cls : type/praxis.layers.linears/Bias
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.dims : 8192
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.name : 'bias'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias._hparams.weight_split_dims_mapping.wt : ['mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.cls : type/praxis.layers.linears/Linear
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.name : 'linear'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.output_dims : 8192
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.activation_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.bias_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.cls : type/praxis.layers.linears/FeedForward
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.has_bias : True
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.input_dims : 8192
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.linear_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.name : 'ffn_layer2'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.output_dims : 2048
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2._hparams.weight_split_dims_mapping.wt : ['mdl', 'data']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.name : 'activation'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.cls : type/praxis.layers.linears/Bias
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.dims : 2048
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.name : 'bias'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias._hparams.weight_split_dims_mapping.wt : ['mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.cls : type/praxis.layers.linears/Linear
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.input_dims : 8192
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.name : 'linear'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.output_dims : 2048
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear._hparams.weight_split_dims_mapping.wt : ['mdl', 'data']
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.cls : type/praxis.layers.normalizations/LayerNorm
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.dim : 2048
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.epsilon : 1e-05
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.name : 'layer_norm'
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.reductions_in_fp32 : False
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.use_scale : True
lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.name : 'relu_dropout'
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_0.ff_layer.relu_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.name : 'residual_dropout'
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_0.ff_layer.residual_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.cls : type/praxis.layers.normalizations/LayerNorm
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.dim : 2048
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.epsilon : 1e-05
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.name : 'layer_norm'
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.reductions_in_fp32 : False
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.use_scale : True
lm.transformer.repeat.sub.x_layers_0.layer_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.name : 'residual_dropout'
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_0.residual_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.activation_split_dims_mapping.bld : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.activation_split_dims_mapping.blnh : [('replica', 'data'), 'NoneType', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.atten_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.atten_logit_cap : -1.0
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.attention_extra_logit : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.attention_mask_summary : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.cast_rotary_position_emb : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.causal_depthwise_conv1d_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.cls : type/praxis.layers.attentions/DotProductAttention
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.combine_qkv : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.combined_qkv_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.cross_head_post_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.cross_head_pre_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dconv_kernel_size : 3
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dconv_qkv : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.decode_cache : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dim_per_head_v : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dynamic_w_post_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.dynamic_w_pre_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.float32_logits : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.float32_probs : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.float32_value : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.headless_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.hidden_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.internal_enable_per_dim_scale : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.internal_enable_query_scale : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.internal_gshard_gaussian_init : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.left_mul : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.logits_absorb_residual : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.logits_output_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.logits_residual : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.logits_squeeze_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.logits_squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.merge_dw_proj : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.name : 'self_attention'
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.ngrammer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.num_kv_heads : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.output_layer_std : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.output_proj_use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.per_dim_scale_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.probs_absorb_residual : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.probs_output_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.probs_residual : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.probs_squeeze_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.probs_squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.project_logits : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.project_probs : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.pv_einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.pythia_rotary : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.pythia_rotary_position_emb_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.qk_einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.qk_norm : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.qk_norm_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.query_chunk_size : 128
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.relative_bias_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.rotary_position_emb_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.rotate_shared_qk : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.scale_bias : 0.0
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.scale_init : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.scale_logits_by_head_dims : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.scale_qkv_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.scale_query_by_dim_per_head : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.scale_shared_key : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.shared_ov_dim : 0
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.shared_post_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.shared_qk_dim : 0
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.transpose_logits : False
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.use_qk_bias : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.use_rotary_position_emb : True
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.value_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.weight_split_dims_mapping.dconv : ['mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.weight_split_dims_mapping.proj : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.window_size : 256
lm.transformer.repeat.sub.x_layers_0.self_attention._hparams.zero_fully_masked : False
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.name : 'atten_dropout'
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_0.self_attention.atten_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.cls : type/praxis.layers.attentions/DynamicWeightProjection
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dd_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.decompose_dynamic_w : True
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw1_norm_bias_const : 0.0
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw1_norm_bias_init : 1e-06
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw1_norm_cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw_activation_cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw_activation_weights : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw_gate_weights : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw_hidden_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dw_hidden_gate_act_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dynamic_d_hidden_dim : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dynamic_d_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dynamic_d_init.scale : 0.00015
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dynamic_squeeze_ratio : 8
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dynamic_w_hidden_dim : 64
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dynamic_w_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.dynamic_w_init.scale : 0.0003
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.key_input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.learned_dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.merge_dynamic_w_hidden : False
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.merge_projection : True
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.name : 'dyn_w_proj'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.query_input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.summary_verbosity : 9
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.use_dw_cap_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.use_dw_hidden_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.axis : -1
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.dim : 0
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.epsilon : 1e-06
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.name : 'dw1_norm'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.name : 'dw_activation'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.approximate : True
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.name : 'dw_hidden_activation'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw_hidden_activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.cls : type/praxis.layers.normalizations/RmsNorm
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.dim : 128
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.direct_scale : True
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.epsilon : 1e-06
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.intermediate_dtype : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.name : 'k_norm'
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.skip_weight_decay : True
lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.is_output_projection : False
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.name : 'key'
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_0.self_attention.key._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.key.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.is_output_projection : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.name : 'post'
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.absorb_residual : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.cls : type/praxis.layers.attentions/CrossHeadProjection
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dd_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dd_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.decompose_dynamic_w : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw1_norm_bias_const : 0.0
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw1_norm_bias_init : 1e-06
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw1_norm_cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw1_norm_dbias_init : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw_activation_cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw_activation_weights : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw_gate_weights : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw_hidden_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dw_hidden_gate_act_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dynamic_d_hidden_dim : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dynamic_d_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dynamic_d_init.scale : 0.00015
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dynamic_squeeze_ratio : 8
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dynamic_w_hidden_dim : 64
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dynamic_w_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.dynamic_w_init.scale : 0.0003
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.gate_relative_scale : 0.01
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.has_dynamic_w_params : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.init : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.key_input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.learnable_diag : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.learned_dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.left_mul : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.loop_over_dynamic_hd : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.merge_dynamic_w_hidden : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.name : 'post_proj'
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.output_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.query_input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.relative_scale : 0.1
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.residual : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.skip_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.skip_ffn_weight_decay : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.square_dw1_norm_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.squeeze_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.squeeze_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.src_dependent : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.summary_verbosity : 9
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.tgt_dependent : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.transpose : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.use_conv : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.use_dw_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.use_dw_cap_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.use_dw_hidden_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.use_squeeze_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.use_static_w : True
lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.absorb_residual : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.cls : type/praxis.layers.attentions/CrossHeadProjection
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dd_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dd_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.decompose_dynamic_w : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw1_norm_bias_const : 0.0
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw1_norm_bias_init : 1e-06
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw1_norm_cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw1_norm_dbias_init : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw_activation_cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw_activation_weights : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw_gate_weights : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw_hidden_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dw_hidden_gate_act_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dynamic_d_hidden_dim : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dynamic_d_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dynamic_d_init.scale : 0.00015
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dynamic_squeeze_ratio : 8
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dynamic_w_hidden_dim : 64
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dynamic_w_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.dynamic_w_init.scale : 0.0003
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.gate_relative_scale : 0.01
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.has_dynamic_w_params : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.init : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.key_input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.learnable_diag : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.learned_dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.left_mul : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.loop_over_dynamic_hd : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.merge_dynamic_w_hidden : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.name : 'pre_proj'
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.output_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.query_input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.relative_scale : 0.1
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.residual : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.skip_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.skip_ffn_weight_decay : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.square_dw1_norm_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.squeeze_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.squeeze_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.src_dependent : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.summary_verbosity : 9
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.tgt_dependent : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.transpose : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.use_conv : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.use_dw_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.use_dw_cap_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.use_dw_hidden_bias : False
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.use_squeeze_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.use_static_w : True
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.name : 'output_activation'
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.output_activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.name : 'pv_einsum'
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.pv_einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.cls : type/praxis.layers.normalizations/RmsNorm
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.dim : 128
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.direct_scale : True
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.epsilon : 1e-06
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.intermediate_dtype : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.name : 'q_norm'
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.skip_weight_decay : True
lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.name : 'qk_einsum'
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.qk_einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.is_output_projection : False
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.name : 'query'
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_0.self_attention.query._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.query.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.cast_as_fprop_dtype : True
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.cls : type/praxis.layers.embedding_softmax/PythiaRotaryPositionalEmbedding
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.embedding_dims : 128
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.max_timescale : 10000
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.min_timescale : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.name : 'rotary_position_emb'
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.rotary_pct : 0.25
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.rotary_position_emb._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.is_output_projection : False
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.name : 'value'
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_0.self_attention.value._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_0.self_attention.value.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.allow_skip_cross_attention : False
lm.transformer.repeat.sub.x_layers_1._hparams.atten_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_1._hparams.cls : type/praxis.layers.transformers/Transformer
lm.transformer.repeat.sub.x_layers_1._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.cross_atten_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_1._hparams.dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1._hparams.gpt_j_residual : True
lm.transformer.repeat.sub.x_layers_1._hparams.hidden_dims : 8192
lm.transformer.repeat.sub.x_layers_1._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_1._hparams.ln_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1._hparams.name : 'x_layers_1'
lm.transformer.repeat.sub.x_layers_1._hparams.ngrammer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.norm_policy : 'pre'
lm.transformer.repeat.sub.x_layers_1._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1._hparams.packed_input : True
lm.transformer.repeat.sub.x_layers_1._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1._hparams.relu_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_1._hparams.residual_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_1._hparams.residual_droppath_prob : 0.0
lm.transformer.repeat.sub.x_layers_1._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.tr_atten_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.tr_fflayer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1._hparams.use_cross_attention : False
lm.transformer.repeat.sub.x_layers_1._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.activation_split_dims_mapping.ffn0 : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.activation_split_dims_mapping.ffn1 : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.activation_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.add_skip_connection : False
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.apply_padding_first : False
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.chunk_size : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.cls : type/praxis.layers.transformers/TransformerFeedForward
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.fflayer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.has_bias : True
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.hidden_dims : 8192
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.internal_gshard_variance_scaling_fan_in_init : False
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.ln_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.name : 'ff_layer'
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.norm_policy : 'pre'
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.output_dims : 0
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.output_layer_std : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.relu_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.relu_dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.residual_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.residual_dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.residual_droppath_prob : 0.0
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.residual_weight : 1.0
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.use_gated_activation : False
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.weight_split_dims_mapping.ffn0 : ['data', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.weight_split_dims_mapping.ffn1 : ['mdl', 'data']
lm.transformer.repeat.sub.x_layers_1.ff_layer._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.activation_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.bias_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.cls : type/praxis.layers.linears/FeedForward
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.has_bias : True
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.linear_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.name : 'ffn_layer1'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.output_dims : 8192
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.approximate : True
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.name : 'activation'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.cls : type/praxis.layers.linears/Bias
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.dims : 8192
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.name : 'bias'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias._hparams.weight_split_dims_mapping.wt : ['mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.cls : type/praxis.layers.linears/Linear
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.input_dims : 2048
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.name : 'linear'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.output_dims : 8192
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear._hparams.weight_split_dims_mapping.wt : ['data', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.activation_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.bias_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.cls : type/praxis.layers.linears/FeedForward
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.has_bias : True
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.input_dims : 8192
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.linear_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.name : 'ffn_layer2'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.output_dims : 2048
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2._hparams.weight_split_dims_mapping.wt : ['mdl', 'data']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.name : 'activation'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.bias_init : 0.0
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.cls : type/praxis.layers.linears/Bias
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.dims : 2048
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.name : 'bias'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias._hparams.weight_split_dims_mapping.wt : ['mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.activation_split_dims_mapping.out : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.cls : type/praxis.layers.linears/Linear
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.input_dims : 8192
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.name : 'linear'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.output_dims : 2048
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.weight_init : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear._hparams.weight_split_dims_mapping.wt : ['mdl', 'data']
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.cls : type/praxis.layers.normalizations/LayerNorm
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.dim : 2048
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.epsilon : 1e-05
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.name : 'layer_norm'
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.reductions_in_fp32 : False
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.use_scale : True
lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.name : 'relu_dropout'
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_1.ff_layer.relu_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.name : 'residual_dropout'
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_1.ff_layer.residual_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.cls : type/praxis.layers.normalizations/LayerNorm
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.dim : 2048
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.epsilon : 1e-05
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.name : 'layer_norm'
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.reductions_in_fp32 : False
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.use_scale : True
lm.transformer.repeat.sub.x_layers_1.layer_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.name : 'residual_dropout'
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_1.residual_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.activation_split_dims_mapping.bld : [('replica', 'data'), 'NoneType', 'mdl']
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.activation_split_dims_mapping.blnh : [('replica', 'data'), 'NoneType', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.atten_dropout_prob : 0.0
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.atten_logit_cap : -1.0
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.attention_extra_logit : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.attention_mask_summary : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.cast_rotary_position_emb : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.causal_depthwise_conv1d_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.cls : type/praxis.layers.attentions/DotProductAttention
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.combine_qkv : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.combined_qkv_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.cross_head_post_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.cross_head_pre_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dconv_kernel_size : 3
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dconv_qkv : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.decode_cache : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dim_per_head_v : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dropout_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dynamic_w_post_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.dynamic_w_pre_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.float32_logits : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.float32_probs : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.float32_value : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.headless_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.hidden_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.internal_enable_per_dim_scale : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.internal_enable_query_scale : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.internal_gshard_gaussian_init : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.left_mul : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.logits_absorb_residual : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.logits_output_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.logits_residual : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.logits_squeeze_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.logits_squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.merge_dw_proj : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.name : 'self_attention'
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.ngrammer_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.num_kv_heads : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.output_layer_std : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.output_proj_use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.per_dim_scale_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.probs_absorb_residual : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.probs_output_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.probs_residual : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.probs_squeeze_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.probs_squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.project_logits : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.project_probs : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.pv_einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.pythia_rotary : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.pythia_rotary_position_emb_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.qk_einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.qk_norm : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.qk_norm_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.query_chunk_size : 128
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.relative_bias_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.rotary_position_emb_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.rotate_shared_qk : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.scale_bias : 0.0
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.scale_init : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.scale_logits_by_head_dims : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.scale_qkv_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.scale_query_by_dim_per_head : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.scale_shared_key : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.shared_ov_dim : 0
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.shared_post_proj_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.shared_qk_dim : 0
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.transpose_logits : False
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.use_qk_bias : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.use_rotary_position_emb : True
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.value_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.weight_split_dims_mapping.dconv : ['mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.weight_split_dims_mapping.proj : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.window_size : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention._hparams.zero_fully_masked : False
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.cls : type/praxis.layers.stochastics/Dropout
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.dropout_at_eval : False
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.keep_prob : 1.0
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.name : 'atten_dropout'
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.noise_shape : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.noise_shape_broadcast_dims : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.transpose_qk : False
lm.transformer.repeat.sub.x_layers_1.self_attention.atten_dropout._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.cls : type/praxis.layers.attentions/DynamicWeightProjection
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dd_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.decompose_dynamic_w : True
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw1_norm_bias_const : 0.0
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw1_norm_bias_init : 1e-06
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw1_norm_cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw_activation_cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw_activation_weights : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw_gate_weights : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw_hidden_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dw_hidden_gate_act_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dynamic_d_hidden_dim : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dynamic_d_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dynamic_d_init.scale : 0.00015
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dynamic_squeeze_ratio : 8
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dynamic_w_hidden_dim : 64
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dynamic_w_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.dynamic_w_init.scale : 0.0003
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.key_input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.learned_dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.merge_dynamic_w_hidden : False
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.merge_projection : True
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.name : 'dyn_w_proj'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.query_input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.summary_verbosity : 9
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.use_dw_cap_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.use_dw_hidden_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.axis : -1
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.dim : 0
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.epsilon : 1e-06
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.name : 'dw1_norm'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.name : 'dw_activation'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.approximate : True
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.name : 'dw_hidden_activation'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw_hidden_activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.cls : type/praxis.layers.normalizations/RmsNorm
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.dim : 128
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.direct_scale : True
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.epsilon : 1e-06
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.intermediate_dtype : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.name : 'k_norm'
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.skip_weight_decay : True
lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.is_output_projection : False
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.name : 'key'
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_1.self_attention.key._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.key.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.is_output_projection : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.name : 'post'
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.params_init.scale : 0.0018414239093399673
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.absorb_residual : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.cls : type/praxis.layers.attentions/CrossHeadProjection
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dd_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dd_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.decompose_dynamic_w : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw1_norm_bias_const : 0.0
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw1_norm_bias_init : 1e-06
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw1_norm_cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw1_norm_dbias_init : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw_activation_cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw_activation_weights : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw_gate_weights : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw_hidden_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dw_hidden_gate_act_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dynamic_d_hidden_dim : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dynamic_d_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dynamic_d_init.scale : 0.00015
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dynamic_squeeze_ratio : 8
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dynamic_w_hidden_dim : 64
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dynamic_w_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.dynamic_w_init.scale : 0.0003
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.gate_relative_scale : 0.01
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.has_dynamic_w_params : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.init : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.key_input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.learnable_diag : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.learned_dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.left_mul : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.loop_over_dynamic_hd : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.merge_dynamic_w_hidden : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.name : 'post_proj'
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.output_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.query_input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.relative_scale : 0.1
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.residual : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.skip_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.skip_ffn_weight_decay : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.square_dw1_norm_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.squeeze_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.squeeze_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.src_dependent : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.summary_verbosity : 9
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.tgt_dependent : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.transpose : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.use_conv : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.use_dw_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.use_dw_cap_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.use_dw_hidden_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.use_squeeze_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.use_static_w : True
lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.absorb_residual : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.cls : type/praxis.layers.attentions/CrossHeadProjection
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dd_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dd_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.decompose_dynamic_w : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw1_norm_bias_const : 0.0
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw1_norm_bias_init : 1e-06
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw1_norm_cls : type/praxis.layers.normalizations/RmsNormNoScale
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw1_norm_dbias_init : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw_activation_cls : type/praxis.layers.activations/Tanh
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw_activation_weights : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw_gate_weights : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw_hidden_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dw_hidden_gate_act_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dynamic_d_hidden_dim : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dynamic_d_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dynamic_d_init.scale : 0.00015
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dynamic_squeeze_ratio : 8
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dynamic_w_hidden_dim : 64
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dynamic_w_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.dynamic_w_init.scale : 0.0003
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.gate_relative_scale : 0.01
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.has_dynamic_w_params : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.init : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.key_input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.learnable_diag : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.learned_dw_cap : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.left_mul : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.loop_over_dynamic_hd : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.merge_dynamic_w_hidden : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.name : 'pre_proj'
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.num_groups : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.output_activation_cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.query_input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.relative_scale : 0.1
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.residual : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.skip_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.skip_ffn_weight_decay : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.square_dw1_norm_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.squeeze_activation_cls : type/praxis.layers.activations/GELU
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.squeeze_gate_activation_cls : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.squeeze_ratio : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.src_dependent : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.summary_verbosity : 9
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.tgt_dependent : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.transpose : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.use_conv : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.use_dw_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.use_dw_cap_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.use_dw_hidden_bias : False
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.use_squeeze_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.use_static_w : True
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.cls : type/praxis.layers.activations/Identity
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.name : 'output_activation'
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.output_activation._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.name : 'pv_einsum'
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.pv_einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.cls : type/praxis.layers.normalizations/RmsNorm
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.dim : 128
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.direct_scale : True
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.epsilon : 1e-06
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.intermediate_dtype : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.name : 'q_norm'
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.skip_weight_decay : True
lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.name : 'qk_einsum'
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.qk_einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.is_output_projection : False
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.name : 'query'
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_1.self_attention.query._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.query.einsum._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.cast_as_fprop_dtype : True
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.cls : type/praxis.layers.embedding_softmax/PythiaRotaryPositionalEmbedding
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.embedding_dims : 128
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.max_timescale : 10000
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.min_timescale : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.name : 'rotary_position_emb'
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.rotary_pct : 0.25
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.rotary_position_emb._hparams.weight_split_dims_mapping.wt : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.attention_combine_dims : False
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.cls : type/praxis.layers.attentions/AttentionProjection
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.dim_per_head : 128
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.dim_per_shared_head : 1
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.einsum_tpl : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.explicit_fan_in_fan_out_axes : False
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.input_dim : 2048
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.is_output_projection : False
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.name : 'value'
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.num_groups : 0
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.num_heads : 16
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.num_shared_heads : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.skip_bias_decay : True
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.use_bias : True
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.use_nhd_shape : False
lm.transformer.repeat.sub.x_layers_1.self_attention.value._hparams.weight_split_dims_mapping.wt : ['data', 'mdl', 'NoneType']
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.activation_split_dims_mapping.out : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.cls : type/praxis.layers.base_ops/EinsumOp
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.contiguous_submeshes : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.dcn_mesh_shape : (1, 1, 1)
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.dtype : type/jax.numpy/float32
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.fprop_dtype : dtype[bfloat16]
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.ici_mesh_shape : (1, 16, 2)
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.mesh_axis_names : ('replica', 'data', 'mdl')
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.name : 'einsum'
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.params_init.method : 'gaussian'
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.params_init.scale : 0.013975424859373685
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.shared_weight_layer_id : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.skip_lp_regularization : NoneType
lm.transformer.repeat.sub.x_layers_1.self_attention.value.einsum._hparams.weight_split_dims_mapping.wt : NoneType


params.lm.embedding_lookup.emb_var.dtype : type/jax.numpy/float32
params.lm.embedding_lookup.emb_var.fan_in_axes : NoneType
params.lm.embedding_lookup.emb_var.fan_out_axes : NoneType
params.lm.embedding_lookup.emb_var.init.method : 'gaussian'
params.lm.embedding_lookup.emb_var.init.scale : 0.013975424859373685
params.lm.embedding_lookup.emb_var.mesh_shape : [1, 16, 2]
params.lm.embedding_lookup.emb_var.repeat_optimizer_dims_mapping : NoneType
params.lm.embedding_lookup.emb_var.repeat_prefix : NoneType
params.lm.embedding_lookup.emb_var.repeat_prefix_split_dims_mapping : NoneType
params.lm.embedding_lookup.emb_var.shape : [50432, 2048]
params.lm.embedding_lookup.emb_var.tensor_split_dims_mapping : ['data', 'mdl']
params.lm.final_ln.bias.collections : ['__lingvo_jax_skip_regularization']
params.lm.final_ln.bias.dtype : type/jax.numpy/float32
params.lm.final_ln.bias.fan_in_axes : NoneType
params.lm.final_ln.bias.fan_out_axes : NoneType
params.lm.final_ln.bias.init.method : 'constant'
params.lm.final_ln.bias.init.scale : 0.0
params.lm.final_ln.bias.mesh_shape : [1, 16, 2]
params.lm.final_ln.bias.repeat_optimizer_dims_mapping : NoneType
params.lm.final_ln.bias.repeat_prefix : NoneType
params.lm.final_ln.bias.repeat_prefix_split_dims_mapping : NoneType
params.lm.final_ln.bias.shape : [2048]
params.lm.final_ln.bias.tensor_split_dims_mapping : [-1]
params.lm.final_ln.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.final_ln.scale.dtype : type/jax.numpy/float32
params.lm.final_ln.scale.fan_in_axes : NoneType
params.lm.final_ln.scale.fan_out_axes : NoneType
params.lm.final_ln.scale.init.method : 'constant'
params.lm.final_ln.scale.init.scale : 0.0
params.lm.final_ln.scale.mesh_shape : [1, 16, 2]
params.lm.final_ln.scale.repeat_optimizer_dims_mapping : NoneType
params.lm.final_ln.scale.repeat_prefix : NoneType
params.lm.final_ln.scale.repeat_prefix_split_dims_mapping : NoneType
params.lm.final_ln.scale.shape : [2048]
params.lm.final_ln.scale.tensor_split_dims_mapping : [-1]
params.lm.softmax.logits_ffn.linear.w.dtype : type/jax.numpy/float32
params.lm.softmax.logits_ffn.linear.w.fan_in_axes : NoneType
params.lm.softmax.logits_ffn.linear.w.fan_out_axes : NoneType
params.lm.softmax.logits_ffn.linear.w.init.method : 'gaussian'
params.lm.softmax.logits_ffn.linear.w.init.scale : 0.013975424859373685
params.lm.softmax.logits_ffn.linear.w.mesh_shape : [1, 16, 2]
params.lm.softmax.logits_ffn.linear.w.repeat_optimizer_dims_mapping : NoneType
params.lm.softmax.logits_ffn.linear.w.repeat_prefix : NoneType
params.lm.softmax.logits_ffn.linear.w.repeat_prefix_split_dims_mapping : NoneType
params.lm.softmax.logits_ffn.linear.w.shape : [2048, 50432]
params.lm.softmax.logits_ffn.linear.w.tensor_split_dims_mapping : ['data', 'mdl']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.shape : [8192]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.bias.b.tensor_split_dims_mapping : ['mdl']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.shape : [2048, 8192]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer1.linear.w.tensor_split_dims_mapping : ['data', 'mdl']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.bias.b.tensor_split_dims_mapping : ['mdl']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.init.scale : 0.0018414239093399673
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.shape : [8192, 2048]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.ffn_layer2.linear.w.tensor_split_dims_mapping : ['mdl', 'data']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.bias.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_0.ff_layer.layer_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.bias.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_0.layer_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.init.scale : 0.00015
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.shape : [2048, 1, 64]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dd.tensor_split_dims_mapping : ['data', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.init.scale : 0.030772872744833184
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.shape : [2048, 1, 4, 64]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.dw1.tensor_split_dims_mapping : ['data', 'NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.init.scale : 0.0003
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.shape : [1, 4, 64, 4, 16]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.dyn_w_proj.qkw.tensor_split_dims_mapping : ['mdl', 'NoneType', 'NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.init.scale : 1.0
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.shape : [128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.k_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.shape : [16, 128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.b.tensor_split_dims_mapping : ['mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.key.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.b.tensor_split_dims_mapping : ['data']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.init.scale : 0.0018414239093399673
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.init.scale : 0.025
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.shape : [1, 16, 16]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.post_proj.w.tensor_split_dims_mapping : ['NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.init.scale : 0.025
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.shape : [1, 16, 16]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.pre_proj.w.tensor_split_dims_mapping : ['NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.init.scale : 1.0
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.shape : [128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.q_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.shape : [16, 128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.b.tensor_split_dims_mapping : ['mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.query.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.shape : [16, 128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.b.tensor_split_dims_mapping : ['mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_0.self_attention.value.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.shape : [8192]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.bias.b.tensor_split_dims_mapping : ['mdl']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.shape : [2048, 8192]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer1.linear.w.tensor_split_dims_mapping : ['data', 'mdl']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.bias.b.tensor_split_dims_mapping : ['mdl']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.init.scale : 0.0018414239093399673
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.shape : [8192, 2048]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.ffn_layer2.linear.w.tensor_split_dims_mapping : ['mdl', 'data']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.bias.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_1.ff_layer.layer_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.bias.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_1.layer_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.init.scale : 0.00015
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.shape : [2048, 1, 64]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dd.tensor_split_dims_mapping : ['data', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.init.scale : 0.030772872744833184
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.shape : [2048, 1, 4, 64]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.dw1.tensor_split_dims_mapping : ['data', 'NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.init.scale : 0.0003
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.shape : [1, 4, 64, 4, 16]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.dyn_w_proj.qkw.tensor_split_dims_mapping : ['mdl', 'NoneType', 'NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.init.scale : 1.0
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.shape : [128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.k_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.shape : [16, 128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.b.tensor_split_dims_mapping : ['mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.key.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.shape : [2048]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.b.tensor_split_dims_mapping : ['data']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.init.scale : 0.0018414239093399673
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.init.scale : 0.025
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.shape : [1, 16, 16]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.post_proj.w.tensor_split_dims_mapping : ['NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.init.scale : 0.025
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.shape : [1, 16, 16]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.pre_proj.w.tensor_split_dims_mapping : ['NoneType', 'NoneType', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.init.scale : 1.0
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.shape : [128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.q_norm.scale.tensor_split_dims_mapping : [-1]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.shape : [16, 128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.b.tensor_split_dims_mapping : ['mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.query.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.collections : ['__lingvo_jax_skip_regularization']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.init.method : 'constant'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.init.scale : 0.0
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.shape : [16, 128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.b.tensor_split_dims_mapping : ['mdl', 'NoneType']
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.dtype : type/jax.numpy/float32
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.fan_in_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.fan_out_axes : NoneType
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.init.method : 'gaussian'
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.init.scale : 0.013975424859373685
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.mesh_shape : [1, 16, 2]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.repeat_optimizer_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.repeat_prefix : [12]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.repeat_prefix_split_dims_mapping : (-1,)
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.shape : [2048, 16, 128]
params.lm.transformer.repeat.sub.x_layers_1.self_attention.value.w.tensor_split_dims_mapping : ['data', 'mdl', 'NoneType']
